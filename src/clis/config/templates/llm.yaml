# =============================================================================
# CLIS LLM Configuration
# =============================================================================
# This file configures which LLM provider and model to use.
# =============================================================================

# LLM Provider
# Supported providers: openai, anthropic, deepseek, qwen, ollama
provider: deepseek

# API Configuration
api:
  # API Key
  # Use environment variable for security (recommended)
  # Set in your shell: export DEEPSEEK_API_KEY="your-key-here"
  key: ${DEEPSEEK_API_KEY}
  
  # API Base URL
  # Change this if using a proxy or custom endpoint
  base_url: https://api.deepseek.com/v1
  
  # Request timeout in seconds
  timeout: 30

# Model Configuration
model:
  # Model name
  # For DeepSeek: deepseek-chat, deepseek-coder
  # For OpenAI: gpt-4, gpt-3.5-turbo
  # For Anthropic: claude-3-opus, claude-3-sonnet
  # For Qwen: qwen-plus, qwen-turbo
  # For Ollama: llama3, codellama, mistral, etc.
  name: deepseek-chat
  
  # Temperature
  # Controls randomness: 0.0 = deterministic, 1.0 = creative
  temperature: 0.1
  
  # Max tokens
  # Maximum tokens in the response
  max_tokens: 2000
  
  # Context Window Configuration
  # Configure how large files are handled based on model's context window
  context:
    # Context window size in tokens
    # Set based on your model:
    #
    # DeepSeek (API):
    #   - deepseek-chat: 64000
    #   - deepseek-coder: 128000
    #   - deepseek-reasoner: 64000
    #
    # Qwen (API):
    #   - qwen-plus: 32000
    #   - qwen-turbo: 8000
    #   - qwen-max: 128000
    #
    # Ollama (local, depends on model):
    #   - llama3: 8192
    #   - llama3.1/3.2: 128000
    #   - codellama: 16384
    #   - mistral/mixtral: 32000
    #   - qwen2/qwen2.5: 32000
    #   - Check your model: ollama show <model> --modelfile
    #
    window_size: 64000
    
    # Enable automatic file chunking
    # When true, large files are automatically split into chunks
    auto_chunk: true
    
    # Manual chunk threshold in tokens
    # 0 = auto (calculated from window_size)
    # Set a specific value to override auto calculation
    chunk_threshold: 0
    
    # Overlap between chunks in lines
    # Helps maintain context between chunks
    chunk_overlap: 200
    
    # Reserved tokens
    # Tokens reserved for system prompt and model response
    reserved_tokens: 4000

# Retry Configuration
retry:
  # Enable automatic retry on failure
  enabled: true
  
  # Maximum retry attempts
  max_attempts: 3
  
  # Delay between retries in seconds
  delay: 1

# Cost Tracking
cost:
  # Enable cost tracking
  # Track API usage and estimated costs
  enabled: true
  
  # Alert threshold in CNY
  # Warn when daily cost exceeds this amount
  daily_threshold: 10.0

# ============================================================================
# Provider-Specific Examples
# ============================================================================

# --- Qwen Example
# provider: qwen
# api:
#   key: ${QWEN_API_KEY}
#   base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
# model:
#   name: qwen-plus
#   temperature: 0.1
#   context:
#     window_size: 32000  # qwen-plus

# --- Ollama (Local) Example
# provider: ollama
# api:
#   base_url: http://localhost:11434
#   # No API key needed for local Ollama
# model:
#   name: llama3
#   # Supported models: llama3, codellama, mistral, qwen2, etc.
#   # Run 'ollama list' to see available models
#   # Check context size: ollama show <model> --modelfile
#   temperature: 0.1
#   context:
#     window_size: 8192  # llama3 default, adjust for your model